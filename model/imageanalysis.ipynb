{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = cv2.imread(\"instagram.jpg\")\n",
    "duplicate = cv2.imread(\"duplicate.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m original_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/HP/OneDrive/Desktop/compare/images/oginsta.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m fakeimage_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/HP/OneDrive/Desktop/compare/images/fakeinsta.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mcompare_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfakeimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mcompare_images\u001b[1;34m(original_path, duplicate_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m duplicate \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(duplicate_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check if images have same size and channels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moriginal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m duplicate\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe images have the same size and channels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Compute pixel-wise difference\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def compare_images(original_path, duplicate_path):\n",
    "    # Load images\n",
    "    original = cv2.imread(original_path)\n",
    "    duplicate = cv2.imread(duplicate_path)\n",
    "\n",
    "    # Check if images have same size and channels\n",
    "    if original.shape == duplicate.shape:\n",
    "        print(\"The images have the same size and channels\")\n",
    "        \n",
    "        # Compute pixel-wise difference\n",
    "        difference = cv2.subtract(original, duplicate)\n",
    "        b, g, r = cv2.split(difference)\n",
    "        \n",
    "        # Check if all channels have zero differences\n",
    "        if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:\n",
    "            print(\"The images are completely equal\")\n",
    "        else:\n",
    "            print(\"The images are not completely equal\")\n",
    "    else:\n",
    "        print(\"The images are not equal in size and channels\")\n",
    "\n",
    "# Example usage\n",
    "original_path = r'C:/Users/HP/OneDrive/Desktop/compare/images/oginsta.jpg'\n",
    "\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/images/fakeinsta.jpg'\n",
    "\n",
    "compare_images(original_path, fakeimage_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m original_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcompare\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mogimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moginsta1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m fakeimage_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/HP/OneDrive/Desktop/compare/images/fakeinsta.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mcompare_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfakeimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mcompare_images\u001b[1;34m(original_path, duplicate_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m duplicate \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(duplicate_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check if images have same size and channels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[43mduplicate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe images have the same size and channels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Compute pixel-wise difference\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def compare_images(original_path, duplicate_path):\n",
    "    # Load images\n",
    "    original = cv2.imread(original_path)\n",
    "    duplicate = cv2.imread(duplicate_path)\n",
    "\n",
    "    # Check if images have same size and channels\n",
    "    if original.shape == duplicate.shape:\n",
    "        print(\"The images have the same size and channels\")\n",
    "        \n",
    "        # Compute pixel-wise difference\n",
    "        difference = cv2.subtract(original, duplicate)\n",
    "        b, g, r = cv2.split(difference)\n",
    "        \n",
    "        # Check if all channels have zero differences\n",
    "        if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:\n",
    "            print(\"The images are completely equal\")\n",
    "        else:\n",
    "            print(\"The images are not completely equal\")\n",
    "    else:\n",
    "        print(\"The images are not equal in size and channels\")\n",
    "\n",
    "# Example usage\n",
    "original_path = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages/oginsta1.jpg'\n",
    "\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/images/fakeinsta.jpg'\n",
    "\n",
    "\n",
    "compare_images(original_path, fakeimage_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of original image 1 is: 0.03 MB\n",
      "The size of original image 2 is: 0.03 MB\n",
      "The size of original image 3 is: 0.04 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def get_file_size(path):\n",
    "    # Get file size in bytes\n",
    "    size_in_bytes = os.path.getsize(path)\n",
    "    # Convert bytes to megabytes\n",
    "    size_in_megabytes = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_megabytes\n",
    "\n",
    "def compare_images(original_paths, duplicate_path):\n",
    "    # Load original images\n",
    "    originals = [cv2.imread(path) for path in original_paths]\n",
    "    \n",
    "    # Get file sizes of original images\n",
    "    original_sizes = [get_file_size(path) for path in original_paths]\n",
    "\n",
    "    # Get file size of duplicate image\n",
    "    duplicate_size = get_file_size(duplicate_path)\n",
    "\n",
    "    # Print sizes of original and duplicate images\n",
    "    for i, original_size in enumerate(original_sizes):\n",
    "        print(f\"The size of original image {i+1} is: {original_size:.2f} MB\")\n",
    "    print(f\"The size of the duplicate image is: {duplicate_size:.2f} MB\")\n",
    "\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path)\n",
    "\n",
    "    # Check if original images have same size and channels\n",
    "    if all(original.shape == duplicate.shape for original in originals):\n",
    "        print(\"The original images have the same size and channels\")\n",
    "        \n",
    "        # Compute Mean Squared Error (MSE) for each pair of images\n",
    "        similarities = []\n",
    "        for original in originals:\n",
    "            mse = ((original - duplicate) ** 2).mean(axis=None)\n",
    "            percentage_match = 100 - (mse / original.size) * 100\n",
    "            similarities.append(percentage_match)\n",
    "\n",
    "        return similarities\n",
    "    else:\n",
    "        print(\"The original images are not equal in size and channels\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "original_paths = [\n",
    "    r'C:/Users/HP/OneDrive/Desktop/compare/ogimages/oginsta1.jpg',\n",
    "    r'C:/Users/HP/OneDrive/Desktop/compare/ogimages/oggoogle.jpg',\n",
    "    r'C:/Users/HP/OneDrive/Desktop/compare/ogimages/ogfacebook.jpg'\n",
    "]\n",
    "\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "match_percentages = compare_images(original_paths, fakeimage_path)\n",
    "if match_percentages is not None:\n",
    "    for i, percentage in enumerate(match_percentages):\n",
    "        print(f\"The similarity between original image {i+1} and duplicate image is: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of original image 1 is: 0.04 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n",
      "The size of original image 1 is: 0.03 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n",
      "The size of original image 1 is: 0.03 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def get_file_size(path):\n",
    "    # Get file size in bytes\n",
    "    size_in_bytes = os.path.getsize(path)\n",
    "    # Convert bytes to megabytes\n",
    "    size_in_megabytes = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_megabytes\n",
    "\n",
    "def compare_images(original_paths, duplicate_path):\n",
    "    # Load original images\n",
    "    originals = [cv2.imread(path) for path in original_paths]\n",
    "    \n",
    "    # Get file sizes of original images\n",
    "    original_sizes = [get_file_size(path) for path in original_paths]\n",
    "\n",
    "    # Get file size of duplicate image\n",
    "    duplicate_size = get_file_size(duplicate_path)\n",
    "\n",
    "    # Print sizes of original and duplicate images\n",
    "    for i, original_size in enumerate(original_sizes):\n",
    "        print(f\"The size of original image {i+1} is: {original_size:.2f} MB\")\n",
    "    print(f\"The size of the duplicate image is: {duplicate_size:.2f} MB\")\n",
    "\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path)\n",
    "\n",
    "    # Check if original images have same size and channels\n",
    "    if all(original.shape == duplicate.shape for original in originals):\n",
    "        print(\"The original images have the same size and channels\")\n",
    "        \n",
    "        # Compute Mean Squared Error (MSE) for each pair of images\n",
    "        similarities = []\n",
    "        for original in originals:\n",
    "            mse = ((original - duplicate) ** 2).mean(axis=None)\n",
    "            percentage_match = 100 - (mse / original.size) * 100\n",
    "            similarities.append(percentage_match)\n",
    "\n",
    "        # Compute overall similarity score\n",
    "        overall_similarity_score = sum(similarities) / len(similarities)\n",
    "        return overall_similarity_score\n",
    "    else:\n",
    "        print(\"The original images are not equal in size and channels\")\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity(originals_dir, duplicate_path):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = []\n",
    "    for original_file in original_files:\n",
    "        similarity_score = compare_images([original_file], duplicate_path)\n",
    "        if similarity_score is not None:\n",
    "            similarity_scores.append(similarity_score)\n",
    "\n",
    "    # Compute average similarity score\n",
    "    if similarity_scores:\n",
    "        average_similarity_score = sum(similarity_scores) / len(similarity_scores)\n",
    "        return average_similarity_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "average_similarity_score = scan_for_similarity(originals_directory, fakeimage_path)\n",
    "if average_similarity_score is not None:\n",
    "    print(f\"The average similarity score between all original images and the duplicate image is: {average_similarity_score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of original image 1 is: 0.04 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n",
      "The size of original image 1 is: 0.03 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n",
      "The size of original image 1 is: 0.03 MB\n",
      "The size of the duplicate image is: 0.06 MB\n",
      "The original images are not equal in size and channels\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def get_file_size(path):\n",
    "    # Get file size in bytes\n",
    "    size_in_bytes = os.path.getsize(path)\n",
    "    # Convert bytes to megabytes\n",
    "    size_in_megabytes = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_megabytes\n",
    "\n",
    "def compare_images(original_paths, duplicate_path):\n",
    "    # Load original images\n",
    "    originals = [cv2.imread(path) for path in original_paths]\n",
    "    \n",
    "    # Get file sizes of original images\n",
    "    original_sizes = [get_file_size(path) for path in original_paths]\n",
    "\n",
    "    # Get file size of duplicate image\n",
    "    duplicate_size = get_file_size(duplicate_path)\n",
    "\n",
    "    # Print sizes of original and duplicate images\n",
    "    for i, original_size in enumerate(original_sizes):\n",
    "        print(f\"The size of original image {i+1} is: {original_size:.2f} MB\")\n",
    "    print(f\"The size of the duplicate image is: {duplicate_size:.2f} MB\")\n",
    "\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path)\n",
    "\n",
    "    # Check if original images have same size and channels\n",
    "    if all(original.shape == duplicate.shape for original in originals):\n",
    "        print(\"The original images have the same size and channels\")\n",
    "        \n",
    "        # Compute Mean Squared Error (MSE) for each pair of images\n",
    "        similarities = []\n",
    "        for original in originals:\n",
    "            mse = ((original - duplicate) ** 2).mean(axis=None)\n",
    "            percentage_match = 100 - (mse / original.size) * 100\n",
    "            similarities.append(percentage_match)\n",
    "\n",
    "        # Compute overall similarity score\n",
    "        overall_similarity_score = sum(similarities) / len(similarities)\n",
    "        return overall_similarity_score\n",
    "    else:\n",
    "        print(\"The original images are not equal in size and channels\")\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity(originals_dir, duplicate_path):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = []\n",
    "    for original_file in original_files:\n",
    "        similarity_score = compare_images([original_file], duplicate_path)\n",
    "        if similarity_score is not None:\n",
    "            similarity_scores.append(similarity_score)\n",
    "\n",
    "    # Compute average similarity score\n",
    "    if similarity_scores:\n",
    "        average_similarity_score = sum(similarity_scores) / len(similarity_scores)\n",
    "        return average_similarity_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def similarity_score_to_percentage(similarity_score):\n",
    "    return similarity_score * 100\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "average_similarity_score = scan_for_similarity(originals_directory, fakeimage_path)\n",
    "if average_similarity_score is not None:\n",
    "    similarity_percentage = similarity_score_to_percentage(average_similarity_score)\n",
    "    print(f\"The average similarity score between all original images and the duplicate image is: {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between original image 1 and the duplicate image is: 23.40%\n",
      "The similarity score between original image 2 and the duplicate image is: 19.20%\n",
      "The similarity score between original image 3 and the duplicate image is: 52.60%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path):\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if duplicate is None:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Compute keypoints and descriptors for the duplicate image\n",
    "    kp2, des2 = orb.detectAndCompute(duplicate, None)\n",
    "\n",
    "    if des2 is None:\n",
    "        print(\"Error: Unable to compute keypoints and descriptors for the duplicate image\")\n",
    "        return None\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        # Load original image\n",
    "        original = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original is None:\n",
    "            print(f\"Error: Unable to load the original image from {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Compute keypoints and descriptors for the original image\n",
    "        kp1, des1 = orb.detectAndCompute(original, None)\n",
    "\n",
    "        if des1 is None:\n",
    "            print(f\"Error: Unable to compute keypoints and descriptors for the original image {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize brute force matcher\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Sort matches by distance\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Compute similarity score\n",
    "        num_matches = len(matches)\n",
    "        total_matches = max(len(kp1), len(kp2))\n",
    "        similarity_score = (num_matches / total_matches) * 100\n",
    "        similarities.append(similarity_score)\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = compare_images_with_content(original_files, duplicate_path)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path)\n",
    "if similarity_scores is not None:\n",
    "    for i, score in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {i+1} and the duplicate image is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between original image C:/Users/HP/OneDrive/Desktop/compare/ogimages\\ogfacebook.jpg and the duplicate image is: 23.40%\n",
      "The similarity score between original image C:/Users/HP/OneDrive/Desktop/compare/ogimages\\oggoogle.jpg and the duplicate image is: 19.20%\n",
      "The similarity score between original image C:/Users/HP/OneDrive/Desktop/compare/ogimages\\oginsta1.jpg and the duplicate image is: 52.60%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path):\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if duplicate is None:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Compute keypoints and descriptors for the duplicate image\n",
    "    kp2, des2 = orb.detectAndCompute(duplicate, None)\n",
    "\n",
    "    if des2 is None:\n",
    "        print(\"Error: Unable to compute keypoints and descriptors for the duplicate image\")\n",
    "        return None\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        # Load original image\n",
    "        original = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original is None:\n",
    "            print(f\"Error: Unable to load the original image from {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Compute keypoints and descriptors for the original image\n",
    "        kp1, des1 = orb.detectAndCompute(original, None)\n",
    "\n",
    "        if des1 is None:\n",
    "            print(f\"Error: Unable to compute keypoints and descriptors for the original image {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize brute force matcher\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Sort matches by distance\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Compute similarity score\n",
    "        num_matches = len(matches)\n",
    "        total_matches = max(len(kp1), len(kp2))\n",
    "        similarity_score = (num_matches / total_matches) * 100\n",
    "        similarities.append((original_path, similarity_score))\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = compare_images_with_content(original_files, duplicate_path)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path)\n",
    "if similarity_scores is not None:\n",
    "    for i, (original_image_name, score) in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {original_image_name} and the duplicate image is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between original image ogfacebook.jpg and the duplicate image is: 23.40%\n",
      "The similarity score between original image oggoogle.jpg and the duplicate image is: 19.20%\n",
      "The similarity score between original image oginsta1.jpg and the duplicate image is: 52.60%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path):\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if duplicate is None:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Compute keypoints and descriptors for the duplicate image\n",
    "    kp2, des2 = orb.detectAndCompute(duplicate, None)\n",
    "\n",
    "    if des2 is None:\n",
    "        print(\"Error: Unable to compute keypoints and descriptors for the duplicate image\")\n",
    "        return None\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        # Load original image\n",
    "        original = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original is None:\n",
    "            print(f\"Error: Unable to load the original image from {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Compute keypoints and descriptors for the original image\n",
    "        kp1, des1 = orb.detectAndCompute(original, None)\n",
    "\n",
    "        if des1 is None:\n",
    "            print(f\"Error: Unable to compute keypoints and descriptors for the original image {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize brute force matcher\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Sort matches by distance\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Compute similarity score\n",
    "        num_matches = len(matches)\n",
    "        total_matches = max(len(kp1), len(kp2))\n",
    "        similarity_score = (num_matches / total_matches) * 100\n",
    "        image_name = os.path.basename(original_path)\n",
    "        similarities.append((image_name, similarity_score))\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = compare_images_with_content(original_files, duplicate_path)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path)\n",
    "if similarity_scores is not None:\n",
    "    for i, (original_image_name, score) in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {original_image_name} and the duplicate image is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between original image ogfacebook.jpg and duplicate image fakeinsta.jpg is: 23.40%\n",
      "The similarity score between original image oggoogle.jpg and duplicate image fakeinsta.jpg is: 19.20%\n",
      "The similarity score between original image oginsta1.jpg and duplicate image fakeinsta.jpg is: 52.60%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path, duplicate_name):\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if duplicate is None:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Compute keypoints and descriptors for the duplicate image\n",
    "    kp2, des2 = orb.detectAndCompute(duplicate, None)\n",
    "\n",
    "    if des2 is None:\n",
    "        print(\"Error: Unable to compute keypoints and descriptors for the duplicate image\")\n",
    "        return None\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        # Load original image\n",
    "        original = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if original is None:\n",
    "            print(f\"Error: Unable to load the original image from {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Compute keypoints and descriptors for the original image\n",
    "        kp1, des1 = orb.detectAndCompute(original, None)\n",
    "\n",
    "        if des1 is None:\n",
    "            print(f\"Error: Unable to compute keypoints and descriptors for the original image {original_path}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize brute force matcher\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Sort matches by distance\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Compute similarity score\n",
    "        num_matches = len(matches)\n",
    "        total_matches = max(len(kp1), len(kp2))\n",
    "        similarity_score = (num_matches / total_matches) * 100\n",
    "        image_name = os.path.basename(original_path)\n",
    "        similarities.append((image_name, duplicate_name, similarity_score))\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path, duplicate_name):\n",
    "    # List all image files in the originals directory\n",
    "    original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Check if there are any images in the directory\n",
    "    if not original_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # Compute similarity score for all images in the directory\n",
    "    similarity_scores = compare_images_with_content(original_files, duplicate_path, duplicate_name)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "duplicate_name = 'fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path, duplicate_name)\n",
    "if similarity_scores is not None:\n",
    "    for i, (original_image_name, duplicate_image_name, score) in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {original_image_name} and duplicate image {duplicate_image_name} is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between original image ogfacebook.jpg and duplicate image fakeinsta.jpg is: 23.40%\n",
      "The similarity score between original image oggoogle.jpg and duplicate image fakeinsta.jpg is: 19.20%\n",
      "The similarity score between original image oginsta1.jpg and duplicate image fakeinsta.jpg is: 52.60%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path, duplicate_name):\n",
    "    # Load duplicate image\n",
    "    duplicate = cv2.imread(duplicate_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if duplicate is None:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Compute keypoints and descriptors for the duplicate image\n",
    "    kp2, des2 = orb.detectAndCompute(duplicate, None)\n",
    "\n",
    "    if des2 is None or len(kp2) == 0:\n",
    "        print(\"Error: Unable to compute keypoints and descriptors for the duplicate image\")\n",
    "        return None\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        try:\n",
    "            # Load original image\n",
    "            original = cv2.imread(original_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if original is None:\n",
    "                print(f\"Error: Unable to load the original image from {original_path}\")\n",
    "                continue\n",
    "\n",
    "            # Compute keypoints and descriptors for the original image\n",
    "            kp1, des1 = orb.detectAndCompute(original, None)\n",
    "\n",
    "            if des1 is None or len(kp1) == 0:\n",
    "                print(f\"Error: Unable to compute keypoints and descriptors for the original image {original_path}\")\n",
    "                continue\n",
    "\n",
    "            # Initialize brute force matcher\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "            # Match descriptors\n",
    "            matches = bf.match(des1, des2)\n",
    "\n",
    "            # Sort matches by distance\n",
    "            matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "            # Compute similarity score\n",
    "            num_matches = len(matches)\n",
    "            total_matches = max(len(kp1), len(kp2))\n",
    "            similarity_score = (num_matches / total_matches) * 100\n",
    "            image_name = os.path.basename(original_path)\n",
    "            similarities.append((image_name, duplicate_name, similarity_score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing image {original_path}: {str(e)}\")\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path, duplicate_name):\n",
    "    try:\n",
    "        # List all image files in the originals directory\n",
    "        original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Check if there are any images in the directory\n",
    "        if not original_files:\n",
    "            print(\"No image files found in the directory.\")\n",
    "            return None\n",
    "\n",
    "        # Compute similarity score for all images in the directory\n",
    "        similarity_scores = compare_images_with_content(original_files, duplicate_path, duplicate_name)\n",
    "\n",
    "        return similarity_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scanning for similarity: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "duplicate_name = 'fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path, duplicate_name)\n",
    "if similarity_scores is not None:\n",
    "    for i, (original_image_name, duplicate_image_name, score) in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {original_image_name} and duplicate image {duplicate_image_name} is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "The similarity score between original image ogfacebook.jpg and duplicate image fakeinsta.jpg is: 78.44%\n",
      "The similarity score between original image oggoogle.jpg and duplicate image fakeinsta.jpg is: 70.91%\n",
      "The similarity score between original image oginsta1.jpg and duplicate image fakeinsta.jpg is: 75.44%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_image(image_path, target_size=(224, 224)):\n",
    "    img = keras_image.load_img(image_path, target_size=target_size)\n",
    "    img_array = keras_image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path, duplicate_name):\n",
    "    # Load duplicate image\n",
    "    try:\n",
    "        duplicate_image = load_image(duplicate_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load pre-trained ResNet50 model\n",
    "    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        try:\n",
    "            # Load original image\n",
    "            original_image = load_image(original_path)\n",
    "\n",
    "            # Get feature vectors for images using the pre-trained model\n",
    "            original_features = model.predict(original_image)\n",
    "            duplicate_features = model.predict(duplicate_image)\n",
    "\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarity_score = cosine_similarity(original_features, duplicate_features)[0][0]\n",
    "            image_name = os.path.basename(original_path)\n",
    "            similarities.append((image_name, duplicate_name, similarity_score * 100))  # Convert similarity score to percentage\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing image {original_path}: {str(e)}\")\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path, duplicate_name):\n",
    "    try:\n",
    "        # List all image files in the originals directory\n",
    "        original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Check if there are any images in the directory\n",
    "        if not original_files:\n",
    "            print(\"No image files found in the directory.\")\n",
    "            return None\n",
    "\n",
    "        # Compute similarity score for all images in the directory\n",
    "        similarity_scores = compare_images_with_content(original_files, duplicate_path, duplicate_name)\n",
    "\n",
    "        return similarity_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scanning for similarity: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "duplicate_name = 'fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path, duplicate_name)\n",
    "if similarity_scores is not None:\n",
    "    for i, (original_image_name, duplicate_image_name, score) in enumerate(similarity_scores):\n",
    "        print(f\"The similarity score between original image {original_image_name} and duplicate image {duplicate_image_name} is: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "The highest similarity score between original image ogfacebook.jpg and duplicate image fakeinsta.jpg is: 78.44%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_image(image_path, target_size=(224, 224)):\n",
    "    img = keras_image.load_img(image_path, target_size=target_size)\n",
    "    img_array = keras_image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "def compare_images_with_content(original_paths, duplicate_path, duplicate_name):\n",
    "    # Load duplicate image\n",
    "    try:\n",
    "        duplicate_image = load_image(duplicate_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Unable to load the duplicate image from {duplicate_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load pre-trained ResNet50 model\n",
    "    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    similarities = []\n",
    "    for original_path in original_paths:\n",
    "        try:\n",
    "            # Load original image\n",
    "            original_image = load_image(original_path)\n",
    "\n",
    "            # Get feature vectors for images using the pre-trained model\n",
    "            original_features = model.predict(original_image)\n",
    "            duplicate_features = model.predict(duplicate_image)\n",
    "\n",
    "            # Compute cosine similarity between feature vectors\n",
    "            similarity_score = cosine_similarity(original_features, duplicate_features)[0][0]\n",
    "            image_name = os.path.basename(original_path)\n",
    "            similarities.append((image_name, duplicate_name, similarity_score * 100))  # Convert similarity score to percentage\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing image {original_path}: {str(e)}\")\n",
    "\n",
    "    if similarities:\n",
    "        return similarities\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scan_for_similarity_with_content(originals_dir, duplicate_path, duplicate_name):\n",
    "    try:\n",
    "        # List all image files in the originals directory\n",
    "        original_files = [os.path.join(originals_dir, file) for file in os.listdir(originals_dir) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Check if there are any images in the directory\n",
    "        if not original_files:\n",
    "            print(\"No image files found in the directory.\")\n",
    "            return None\n",
    "\n",
    "        # Compute similarity score for all images in the directory\n",
    "        similarity_scores = compare_images_with_content(original_files, duplicate_path, duplicate_name)\n",
    "\n",
    "        return similarity_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scanning for similarity: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "originals_directory = r'C:/Users/HP/OneDrive/Desktop/compare/ogimages'\n",
    "fakeimage_path = r'C:/Users/HP/OneDrive/Desktop/compare/fake images/fakeinsta.jpg'\n",
    "duplicate_name = 'fakeinsta.jpg'\n",
    "\n",
    "similarity_scores = scan_for_similarity_with_content(originals_directory, fakeimage_path, duplicate_name)\n",
    "if similarity_scores is not None:\n",
    "    similarity_scores.sort(key=lambda x: x[2], reverse=True)  # Sort similarity scores in descending order\n",
    "    highest_similarity = similarity_scores[0]\n",
    "    print(f\"The highest similarity score between original image {highest_similarity[0]} and duplicate image {highest_similarity[1]} is: {highest_similarity[2]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
